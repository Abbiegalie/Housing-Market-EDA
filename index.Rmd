---
title: 'Project 1: Wrangling, Exploration, Visualization'
author: "SDS322E"
date: 11/03/2021
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
    
    ## Data Wrangling, Exploration, Visualization

### Abigail Tovar art2976


---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
```
#### Introduction 
#This project analyzes local house prices around Austin Texas. Austin is one of the fastest growing cities in the nation proving itself to be a desirable spot for travel, leisure, and work in the past decade. The second dataset I analyzed for the project contains data of crime in Austin in 2018. This phenomenon is interesting to me as I have lived here for many years so seeing the transition within real estate from gentrification, and the implications of these changing demographics in certain areas over others in terms of crime rate is of great interest to me. Furthermore, there was a good amount of quality data that was readily available and highly relatable for these topics. I acquired the two datasets from Kaggle.com,  as well as, the official website for the city of Austin government.


```{R eval=F}
 
#2. Data prep
#I performed an inner_join on the datasets because I wanted to preserve the data that was in the second data set, without dropping what was not in the first data set. I joined based on the zipcode for each area and eventually I figured it help me to visualize the distribution of prices in the area. After joining, the joined data set had 64 variables, and 43,141,748 objects whereas before there were 102446 objects and 27 variables in the crime dataset and 47 variables and 15,318 in the "Austin Housing dataset".

library(tidyverse)
data_1 <- read_csv("Crime_Reports_2018.csv")
data_2 <- read_csv("austinHousingData.csv")
merged_data <- data_1 %>% inner_join(data_2, by = c('Zip Code' = "zipcode"))

```

```{R eval=F}
#2. Data Prep Continued.. Drop Rows which are unnecessary and remove duplicates
merged_data_ <- merged_data %>% select(-c(latitude, longitude, numOfPhotos, homeImage, avgSchoolDistance, avgSchoolSize, latestPriceSource, hasSpa, numOfAccessibilityFeatures, numOfParkingFeatures))
```

```{R eval=F}
#Find the Household that has the greatest listing price, then summarize by the zipcode and find the average price of the houses in this area. To find the most expensive area in Austin I grouped by zipcode and then summarized the columns in terms of average house prices in descending order. The result is west lake hills or the '78746' zipcode. Create a variable that stores the cumulative sum of houses for each zipcode.

merged_data_3 <- merged_data_ %>% distinct(`zpid`, .keep_all = TRUE)
merged_data_3 %>% arrange(desc(latestPrice))
merged_data_3 %>% group_by(`Zip Code`) %>% summarize(average_price = mean(latestPrice)) %>% arrange(desc(average_price))

```

```{R eval=F}

#remove duplicate case numbers from dataset
crime_no_dup = merged_data_ %>% distinct(Address, .keep_all=TRUE)

#Find the cumulative crime rate by zipcode, using the crime dataset and sort by highest, which gives the zipcode of 78745, an area in South Austin. This state the cumulative crime amount was 2640. Then to find the most common type of offense use the count feature, which reveals burglary of vehicle is most common with 317 reported crimes in 2018.
crime_no_dup %>% group_by(`Zip Code`, `Highest Offense Description`) %>% summarize(count=n()) %>% filter(`Zip Code` == 78745) %>%  mutate("cm_crime"= max(cumsum(count))) %>% arrange(desc(count))

```



```{R eval=F}
library(gt)

#Summarize Data and Count. 

#Summarize some the numeric variables: 'latestPrice', 'numPriceChanges', 'yearBuilt', 'latestSaleYear', 'avgSchoolRating')
crime_no_dup %>% na.omit() %>% summarize(mean = mean(latestPrice, na.rm = TRUE), sd = sd(latestPrice, na.rm = TRUE), quantile = quantile(latestPrice, probs = c(.25, .75), na.rm = TRUE), n_rows = n(), var = var(latestPrice,  na.rm = TRUE), n_distinct = n_distinct(latestPrice), min = min(latestPrice,  na.rm = TRUE), max = max(latestPrice,  na.rm = TRUE))

crime_no_dup %>% na.omit() %>% summarize(mean = mean(numPriceChanges, na.rm = TRUE), sd = sd(numPriceChanges, na.rm = TRUE), quantile = quantile(numPriceChanges, probs = c(.25, .75), na.rm = TRUE), n_rows = n(), var = var(numPriceChanges,  na.rm = TRUE), n_distinct = n_distinct(numPriceChanges), min = min(numPriceChanges,  na.rm = TRUE), max = max(numPriceChanges,  na.rm = TRUE)) 

crime_no_dup %>% na.omit() %>% summarize(mean = mean(yearBuilt, na.rm = TRUE), sd = sd(yearBuilt, na.rm = TRUE), quantile = quantile(yearBuilt, probs = c(.25, .75), na.rm = TRUE), n_rows = n(), var = var(yearBuilt,  na.rm = TRUE), n_distinct = n_distinct(yearBuilt), min = min(yearBuilt,  na.rm = TRUE), max = max(yearBuilt,  na.rm = TRUE)) 

crime_no_dup %>% na.omit() %>% summarize(mean = mean(avgSchoolRating, na.rm = TRUE), sd = sd(avgSchoolRating, na.rm = TRUE), quantile = quantile(avgSchoolRating, probs = c(.25, .75), na.rm = TRUE), n_rows = n(), var = var(avgSchoolRating,  na.rm = TRUE), n_distinct = n_distinct(avgSchoolRating), min = min(avgSchoolRating,  na.rm = TRUE), max = max(avgSchoolRating,  na.rm = TRUE)) 

#Summarize some of the the categorical variables
crime_no_dup %>% group_by(`Category Description`) %>% count()
crime_no_dup %>% group_by(`Council District`) %>% count()
crime_no_dup %>% group_by(`Highest Offense Description`) %>% count()
crime_no_dup %>% group_by(homeType) %>% count()
#Compute Summary Statistics Overall
summarise_all(crime_no_dup,funs(mean, sd, var, n_distinct, n(), max, min))
crime_no_dup %>% group_by(`Zip Code`) %>% summarize(mean = mean(numOfPrimarySchools)) %>% pivot_wider(names_from = `Zip Code`, values_from = mean)
crime_no_dup %>% summarize_all(function(x)sum(is.na(x)))

#Reshape Tables with Summaries

table1<- crime_no_dup %>% group_by(`Zip Code`) %>% summarize(mean_schools = mean(numOfPrimarySchools+numOfElementarySchools+numOfHighSchools+numOfMiddleSchools)) %>% gt()
table1
table_2 <- tibble(summarise_all(crime_no_dup,funs(mean, sd, var, n_distinct, n(), max, min)))


```

```{R eval=F}
library(ggplot2)

# In this project, I analyzed the criminal incident data from the year of 2018 for the city of Austin. The first visualization is a graphical representation of the relationship between the variables for the incident reports and Austin City Council Districts. It can be find the place with the greatest amount of incident reports, is District 1, or far East Austin, and the place with the least is District 6, or far Northwest Austin. Overall the trend of the graph seems to prove that whether North or South, the most dangerous neighborhoods in Austin are within East Austin, and i-35 is somewhat of the dividing line.

crime_daily = crime_no_dup %>% group_by(`Council District`, `Incident Number`)
crime_count = summarize(crime_daily) %>% count()
ggplot(crime_count, aes(x=`Council District`, y=n)) + geom_point(colour = "red") + 
  geom_line(colour = "red", size = 1.5) + theme_light(base_size = 12) + xlab("Coucil District") + ylab("Count of indicents") +   
  scale_x_continuous(breaks=c(1:10)) +
  ggtitle("The Number of Incidents in Austin Council Districts") + 
  theme(plot.title=element_text(size=16))

# In the project, I analyzed the crime incident data from the year of 2018 for the city of Austin. The first visualization is a graphical representation of the relationship between the variables for the incident reports and Austin City Council Districts. It can be find the place with the greatest amount of incident reports, is District 1, or far East Austin, and the place with the least is District 6, or far Northwest Austin. Overall the trend of the graph seems to prove that whether North or South, the most dangerous neighborhoods in Austin are within East Austin, and i-35 is somewhat of the dividing line.

merged_data_3 %>% group_by(`Council District`) %>% summarize(average_price = mean(latestPrice)) %>% na.omit() %>% ggplot(merged_data_3, mapping = aes(x = `Council District`, y = average_price)) +  geom_bar(stat= "identity") + scale_x_continuous(breaks = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)) 

# This bar graph of the recent listings values of sold properties around Austin sorted by the Austin City Council Districts shows that the areas with generally higher home values have been safer if we look back to the graph that analyzed the crime incident data and compare districts. This could potentially show that higher crime rates can decrease the proprety values in an area. Another observation which can be made within the City Council District 9, where UT Austin is located, there are the highest average listing sale prices in Austin.
category_count = crime_no_dup %>% group_by(`Category Description`) %>% count()
crime_no_dup %>% drop_na() %>% ggplot(crime_no_dup, mapping = aes(x = frequency(category_count), y =`Category Description`)) + geom_col() + xlab("Frequency") +  ggtitle("Freqeuncies of Different types of Crime in Austin 2018") + theme(plot.title=element_text(size=16))
 
# This bar graph has bars which represent the overall amount of crime in Austin in the year 2018 in different categories, using the crime dataset and removing the many NAs. From the available data that had been categorized it seems theft was the most common crime by far. Following general theft, are the categories for crime of burglary and auto-theft.  
```

