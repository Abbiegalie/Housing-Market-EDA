---
title: 'Project 1: Wrangling, Exploration, Visualization'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
```

## Data Wrangling, Exploration, Visualization

### Abigail Tovar

#### Introduction 
This project analyzes local house prices around Austin Texas. Austin is one of the fastest growing cities in the nation proving itself to be a desirable spot for travel, leisure, and work in the past decade. The second dataset I analyzed for the project contains data of crime in Austin in 2018. This phenomenon is interesting to me as I have lived here for many years so seeing the transition within real estate from gentrification, and the implications of these changing demographics in certain areas over others in terms of crime rate is of great interest to me. Furthermore, there was a good amount of quality data that was readily available and highly relatable for these topics. I acquired the two datasets from Kaggle.com,  as well as, the official website for the city of Austin government.





#### Tidying: Reshaping

If your datasets are tidy already, demonstrate that you can reshape data with pivot wider/longer here (e.g., untidy and then retidy). Alternatively, it may be easier to wait until the wrangling section so you can reshape your summary statistics. Note here if you are going to do this.

```{R}
#Will reshape later
```

    
#### Joining/Merging

```{R}

library(tidyverse)
data_1 <- read_csv("Crime_Reports_2018.csv")
data_2 <- read_csv("austinHousingData.csv")
merged_data <- data_1 %>% inner_join(data_2, by = c('Zip Code' = "zipcode"))
head(merged_data)

```

I performed an inner_join on the datasets because I wanted to preserve the data that was in the second data set, without dropping what was not in the first data set. I joined based on the zipcode for each area and eventually I figured it help me to visualize the distribution of prices in the area. After joining, the joined data set had 64 variables, and 43,141,748 objects. The crime dataset had 102446 rows and 27 columns,  26 of which were unique in the crime dataset and 47 variables 46 of which were unique and 15,318 objects in the "Austin Housing dataset".



####  Wrangling

```{R}
library(tidyverse)
library(gt)

merged_data_3 <- merged_data %>% distinct(`zpid`, .keep_all = TRUE)
merged_data_3 %>% arrange(desc(latestPrice))
merged_data_3 %>% group_by(`Zip Code`) %>% summarize(average_price = mean(latestPrice)) %>% arrange(desc(average_price)) %>% gt() %>% tab_header(title=md("**Average Household Prices in Austin by Zipcode**"))

```
Before performing analysis on the data set I needed it to be tidy. Using tidy verse I dropped the rows which were unnecessary and removed deuplicates. Above I found the household that has the greatest listing price, then summarized by the zipcode to find the average price of the houses in this area. To find the most expensive area in Austin I grouped by zipcode and then summarized the columns in terms of average house prices in descending order, I also had to create a variable that stores the cumulative sum of houses for each zipcode. The result of was that the West Lake Hills or the '78746' zipcode was the most expensive area in Austin, which would make sense as it has some of the best views in Austin, the average price for a listing here using my dataset was $1,422,145.10.	

```{R}
crime_no_dup <- merged_data %>% distinct(Address, .keep_all = TRUE)

crime_no_dup %>% group_by(`Zip Code`, `Highest Offense Description`) %>% summarize(count = n()) %>% mutate(cm_crime = max(cumsum(count))) %>% arrange(desc(count))

```

I wanted to use my data to find the area which was most dangerous by zipcode, using the crime dataset and sorting by highest after calculating the cumulative sum, I got the zipcode of 78745, an area in South Austin. This stated the cumulative crime amount was 2640. I also was curoius to see the most common type of offense here which I found using the count feature, which reveals burglary of vehicle was most common with 317 reported crimes in 2018.

```{R}
crime_no_dup %>% summarize(mean = mean(latestPrice, na.rm = TRUE), quantile = quantile(latestPrice, probs = c(0.25, 0.75), na.rm = TRUE), counts = n(), n_distinct = n_distinct(latestPrice), min = min(latestPrice, na.rm = TRUE), max = max(latestPrice, na.rm = TRUE))

crime_no_dup %>% na.omit() %>% summarize(mean = mean(numPriceChanges, na.rm = TRUE), sd = sd(numPriceChanges, na.rm = TRUE), quantile = quantile(numPriceChanges,  probs = c(0.25, 0.75), na.rm = TRUE), counts = n(), var = var(numPriceChanges,  na.rm = TRUE), n_distinct = n_distinct(numPriceChanges),  min = min(numPriceChanges, na.rm = TRUE), max = max(numPriceChanges,  na.rm = TRUE))

crime_no_dup %>% na.omit() %>% summarize(mean = mean(yearBuilt, 
    na.rm = TRUE), sd = sd(yearBuilt, na.rm = TRUE), quantile = quantile(yearBuilt, 
    probs = c(0.25, 0.75), na.rm = TRUE), counts = n(), var = var(yearBuilt, 
    na.rm = TRUE), n_distinct = n_distinct(yearBuilt), min = min(yearBuilt, 
    na.rm = TRUE), max = max(yearBuilt, na.rm = TRUE))

crime_no_dup %>% group_by() %>% summarize(mean = mean(latest_saleyear, 
    na.rm = TRUE), sd = sd(latest_saleyear, na.rm = TRUE), quantile = quantile(latest_saleyear, 
    probs = c(0.25, 0.75), na.rm = TRUE), counts = n(), var = var(latest_saleyear, 
    na.rm = TRUE), n_distinct = n_distinct(yearBuilt), min = min(latest_saleyear, 
    na.rm = TRUE), max = max(latest_saleyear, na.rm = TRUE))

```

The summary statistics above I calculated for latest price, number of price changes, year built and sale year. The first for latest price showed that the mean price was $480,404.90 with a standard deviation of \$273,209.20 which was fairly high I thought which could signify this value was spread over a wider range.The range of selling houses in terms of latest price is \$300000 to \$639,000. The cheapest house was \$194,800 dollars which definitely speaks to how expensive Austin is, especially in this time of growth where prices for houses have surged.  The max number of price changes was 13, and average was 2, which had expected to be higher, assuming that prices have been changing a lot in the market as it is experiencing a growth in demand. Year built for the average house in my dataset was 1990 and the standard deviation was 21 years.




``` {R}
merged_data_3 %>% filter(`Zip Code` == 78746) %>% mutate(address_low = str_to_lower(Address)) %>%  select(Address, address_low) %>% head(5)

address_sample <- merged_data_3 %>% filter(`Zip Code` == 78746) %>% mutate(address_low = str_to_lower(Address)) %>%  select(Address, address_low) %>% head(1)

str_detect(address_sample, "a")


```
Using string r I converted the addresses in the zipcode 78746 to lower case, and saved them as a variable "address_low" then I used str_detect to see if the first address had a lowercase "a". It returned false for the addresses I hadn't converted and true for address_low.

``` {R}

table1 <- crime_no_dup %>% group_by(`Zip Code`, `Location Type`) %>% summarize (count = n()) 


#Reshape tables 
table1 <- crime_no_dup %>% group_by(`Zip Code`) %>% summarize(mean_schools = mean(numOfPrimarySchools + numOfElementarySchools + numOfHighSchools + numOfMiddleSchools)) %>% pivot_wider(names_from = `Zip Code`, values_from = mean_schools)
table1

```
Lastly I summarized the crime statistics overall in terms of the mean, standard deviation, variance, maximum and minimum, as well as, count. I wanted to see areas downtown and what crime stats were here, which was interesting as it revealed in 78701 a lot of crime was around bar and night clubs, parking lots, as well as streets. I used pivot_wider to arrange the summary statistics here for Zip Code and location type which broke down th most common location in which crime occurred in certain areas. I also arranged a table for statistics which showed the number of public schools in each zip code it uses pivot wider to create the summary statistics, the areas with the highest areas of schools were 78753 and 78754. 


#### Visualizing

```{R}
library(ggplot2)
crime_daily = crime_no_dup %>% group_by(`Council District`, `Incident Number`)
crime_count = summarize(crime_daily) %>% count()
graph3 <- ggplot(crime_count, aes(x = `Council District`, y = n)) + geom_point(colour = "red") + 
    geom_line(colour = "red", size = 1.5) + theme_light(base_size = 12) + 
    xlab("Coucil District") + ylab("Count of indicents") + scale_x_continuous(breaks = c(1:10)) + 
    ggtitle("The Number of Incidents in Austin Council Districts") + 
    theme(plot.title = element_text(size = 16))
print(graph3)
```
In this project, I analyzed the crime incident data from the year of 2018 for the city of Austin. The first
visualization is a graphical representation of the relationship between the variables for the incident reports
and Austin City Council Districts. It can be find the place with the greatest amount of incident reports, is District 1, or far East Austin, and the place with the least is District 6, or far Northwest Austin. Overall the trend of the graph seems to prove that whether North or South, the most dangerous neighborhoods in Austin are within East Austin, and i-35 is somewhat of the dividing line.

```{R}
library(ggplot2)
graph2 <- merged_data_3 %>% group_by(`Council District`) %>% summarize(average_price = mean(latestPrice)) %>% 
    na.omit() %>% ggplot(merged_data_3, mapping = aes(x = `Council District`, 
    y = average_price)) + geom_bar(stat = "identity") + scale_x_continuous(breaks = c(1, 
    2, 3, 4, 5, 6, 7, 8, 9, 10))
print(graph2)
```

This bar graph of the recent listings values of sold properties around Austin sorted by the Austin City Council Districts shows that the areas with generally higher home values have been safer if we look back to the graph that
analyzed the crime incident data and compare districts. This could potentially show that higher crime rates can
decrease the proprety values in an area. Another observation which can be made within the City Council
District 9, where UT Austin is located, there are the highest average listing sale prices in Austin.

```{R}
library(ggplot2)
category_count = crime_no_dup %>% group_by(`Category Description`) %>% 
    count()
graph1 <- crime_no_dup %>% drop_na() %>% ggplot(crime_no_dup, mapping = aes(x = frequency(category_count), y = `Category Description`)) + geom_col() + xlab("Frequency") +  ggtitle("Freqeuncies of Different types of Crime in Austin 2018") +  theme(plot.title = element_text(size = 16))
print(graph1)
```

This bar graph has bars which represent the overall amount of crime in Austin in the year 2018 in different
categories, using the crime dataset and removing the many NAs. From the available data that had been categorized it seems theft was the most common crime by far. Following general theft, are the categories for crime of burglary and auto-theft.

##Conclusion
It wasn't surprising to see that there was a correlation between the overall median home values in neighborhoods in Austin and how crime rates in the area, as typically unemployment, educational opportunities, and quality of life can differ greatly just by changing the factor of the home values.


